<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="assets/ico/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Josefin+Sans" rel="stylesheet">

    <title>Jaeyoon Jung - Yelp Data Challenge</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/resume.min.css" rel="stylesheet">

  </head>

  <body>
    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
      <a class="navbar-brand js-scroll-trigger" href="index.html#page-top">
        <span class="d-block d-lg-none">Start Bootstrap</span>
        <span class="d-none d-lg-block">
          <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="img/profile.JPG" alt="">
        </span>
      </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="index.html#about">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="index.html#projects">Projects</a>
          </li>
        </ul>
      </div>
    </nav>

    <header class="masthead" style="background-image: url('img/work.jpg')">
            <div class="container">
                <div class="row">
                    <div class="project-header col-lg-12 col-lg-offset-0">
                        <h1>Yelp Data Challenge</h1>
                        <h4>Sentiment analysis of Yelp's review data</h4>
                    </div>
                </div>
            </div>
    </header>

    <section class="p-3 p-lg-5 d-flex flex-column" id="summary">
      <div class="my-auto" align="center">
        <div class="resume-item d-flex flex-column flex-md-row mb-5 col-lg-10" style="text-align: left;">
          <div class="resume-content mr-auto">
            <h2 class="mb-1">ABOUT THIS PROJECT</h2>
            <br>
            <p>In this project, I built a sentiment analysis model that can classify the sentiment of Yelp users' reviews.</p>
            <br>
            <h4>Data Exploration</h4>
            <p>
                A lot of online platforms that are powered by crowd-sourced reviews today feature numerical ratings. On Yelp, for example, users leave ratings from 1 to 5 stars when writing about their experiences. However, with these very subjective metrics, it can be challenging to gauge what these numbers <em>actually</em> mean.
            </p>
            <p>
                To begin with, with the given scoring metric, it would be reasonable to guess that the average rating would be around 3. In reality, the distribution of Yelp ratings is left-skewed. At a glance, Yelp users seem like an optimistic bunch.
            <ul>
                <li>Total: 4,153,150 Reviews</li>
                <li>1 Star: 540,377 Reviews</li>
                <li>2 Star: 358,550 </li>
                <li>3 Star: 517,369 Reviews</li>
                <li>4 Star: 1,032,654 Reviews</li>
                <li>5 Star: 1,704,200 Reviews</li>
            </ul>
            <img class="img" src="img/yelp/yelp1.png">
            <br>
            <h4>Data Preprocessing</h4>
            <p>
                The entire dataset with 4.1 million reviews is too large for my computer's memory.
                Hence, I read only the frist 500,000 lines of reviews to train the model.
                Since the data itself does not have labels 'positive' and 'negative',
                I assumed that all 1 starred and 5 starred reviews carry negative and positive sentiment,
                respectively. There are 199,676 positive (5-starred) reviews and 70,146 negative (1-starred)
                reviews in the first 500,000 lines.
            </p>
            <p>
                The following table shows most popular words (by count) for each class:
            </p>
            <br>
            <table class="table table-striped">
                <thead class="thead-default">
                    <tr>
                        <th></th>
                        <th>Most Popular Words</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Positive</td>
                        <td>food, well, place, great, best, it, film, one, phone, good, like, really, love</td>
                    </tr>
                    <tr>
                        <td>Negative</td>
                        <td>film, it, get, really, place, good, even, food, phone, bad, would, time, one, like</td>
                    </tr>
                </tbody>
            </table>
            <p>
                Then, these reviews are tokenized after the following formatting:
            </p>
            <ul>
                <li>All characters to lower case</li>
                <li>Repeated characters are reduced to one. This is to prevent having words like 'gooooooood'</li>
                <li>Emojis are stripped</li>
                <li>Punctuations are stripped</li>
                <li>Non-Latin characters are stripped</li>
                <li>All white space is reduced to one</li>
                <li>All numbers are stripped</li>
                <li>English stopwords (list obtained from NLTK module) are stripped</li>
                <li>All words are lemmatized, using WordNet Lemmatizer </li>
                <li>All words are reducted to their stemmers, using Porter Stemmer Algorithm </li>
            </ul>
            <p>
                Then, I also removed words that appear in both classes too frequently, in order to reduce the number of uninformative features. For example, words like food and restaurants appear very frequently in both positive and negative reviews, and they're not helpful in capturing the sentiment of text at all. I identified these common words by taking the inserct of 1000 most frequent words in positive and negative reviews.
            </p>
            <br>
            <h4>Feature Engineering & Selection</h4>
            <p>
                Instead of just using frequency of preprocessed tokens, I used engineered a few features
                that improved classification accuracy.
            </p>
            <ul>
                <li>Bigram: the order of word occurrence could be informative. Bigram is helpful
                    in recreating this without making features too specific to training data.</li>
                <li>TF-IDF (term frequency-inverse document frequency):
                    using TF-IDF values decreases weight on terms that appear
                    very often across the entire corpus. Terms with high document
                    frequency are not very informative and function as corpus-specific stop words.</li>
                <li>Character length & word count: users with strong sentiment may write more than others.</li>
                <li>Upper case count: users with strong sentiment tend to write
                    reviews in upper case to make them dramatic.</li>
            </ul>
            <p>
                Using the above feature extraction results in a giant sparse matrix. As a reference, training on
                2400 reviews results in 3479 columns. To find more meaningful features among 10,000s of features,
                I tried the following two methodologies:
            </p>
            <ul>
                <li>SVD (Singular Value Decomposition): I decomposed the sparse feature matrix into linear
                    combination of independent components. The decomposed matrix is sorted by eigenvalue,
                    which represents variance explained by each eigenvector (component).
                    By taking first 100 columns, I reduced dimensionality of data to remove some noise.
                    Because this transformation generates negative values, I could not use it for Nave Bayes classifiers.
                </li>
                <li>Chi-Square Feature Selection: I used chi-square feature selection as an alternative of SVD.
                    This computes chi-square score of independence between features and classes (positive vs negative)
                    to find 100 best features that are dependent on classes.
                </li>
            </ul>
            <br>
            <h4>Sentiment Analysis</h4>
            <p>
                I tried Bernoulli Naive Bayes, Multinomial Bayes, Decision Tree, Random Forest,
                K Nearest Neighbors, and Support Vector Machine algorithms with different combinations
                of features and parameters. For rigorous evaluation of models, I performed
                5-fold cross validation and compared average accuracy and F1 scores to do final evaluation.
            </p>
            <p>
                My two best performing models were <b>Multinomial Naive Bayes Classifier</b> and <b>Support Vector Machine.</b>
                They both use unigram, bigram, and TF-IDF as features and reduced dimensionality of data by Chi-square score of independece.
            </p>
            <br>
            <br>
            <br>
            <table class="table table-striped">
                <thead class="thead-default">
                    <tr>
                        <th>Classifier</th>
                        <th>Multinomial NB</th>
                        <th>SVM</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Average Accuracy</td>
                        <td>0.819</td>
                        <td>0.818</td>
                    </tr>
                    <tr>
                        <td>F1</td>
                        <td>0.823</td>
                        <td>0.819</td>
                    </tr>
                </tbody>
            </table>
            <br>
            <p>
                According to the results of K-Fold cross validation, the two models are very close
                in accuracy and F1 score. However, SVM has time complexity greater than quadratic,
                and the fit time grew exponentially as features grew. Hence, Multinomial Nave Bayes
                is more appropriate for scaling on a much larger dataset.
            </p>
            <br>
            <table class="table table-striped">
                <thead class="thead-default">
                    <tr>
                        <th></th>
                        <th>Most Informative Features</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Positive</td>
                        <td>well, excel, food, movi, film</td>
                    </tr>
                    <tr>
                        <td>Negative</td>
                        <td>miss numer, miss entir, misplac, mislead, mishima extrem</td>
                    </tr>
                </tbody>
            </table>
            <p>
                I also computed the most informative features for the Multinomial Naive Bayes classifier.
                It seems that basic root words which are popular across all positive samples seem to be
                the most informative features for positive samples.
                However, that is not the case for the negative class. Indeed, the most informative
                features associated with the negative class are much harder to make sense of at face value.
                Firstly, they seem to be morphological negations, not root words. Perhaps they are derived
                from the stemming and lemmatizing steps that occur during preprocessing of the data.
                In any case, the poor overlap that they have with the most popular words group for the
                negative class suggests that grammatical analysis of negative samples is more
                important for their classification than the simple presence of negative buzz words. This makes
                intuitive sense. When people are happy with something, they tend to express it short and sweet, but
                if they are dissatisfied, they will explain precisely what they found to be dissatisfactory (requires
                higher levels of grammatical and semantic parsing).
            </p>
          </div>
        </div>
      </div>
    </section>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="assets/js/bootstrap.min.js"></script>
  </body>
</html>
