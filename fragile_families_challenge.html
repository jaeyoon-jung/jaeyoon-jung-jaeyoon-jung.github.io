<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="assets/ico/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Josefin+Sans" rel="stylesheet">

    <title>Jaeyoon Jung - Fragile Family Challenge</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/resume.min.css" rel="stylesheet">

  </head>

  <body>
    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
      <a class="navbar-brand js-scroll-trigger" href="index.html#page-top">
        <span class="d-block d-lg-none">Start Bootstrap</span>
        <span class="d-none d-lg-block">
          <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="img/profile.JPG" alt="">
        </span>
      </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="index.html#about">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="index.html#projects">Projects</a>
          </li>
        </ul>
      </div>
    </nav>

    <header class="masthead" style="background-image: url('img/work.jpg')">
            <div class="container">
                <div class="row">
                    <div class="project-header col-lg-12 col-lg-offset-0">
                        <h1>Fragile Families Challenge</h1>
                        <h4>Factors that affect the wellbeing of children of fragile families </h4>
                    </div>
                </div>
            </div>
    </header>

    <section class="p-3 p-lg-5 d-flex flex-column" id="summary">
      <div class="my-auto" align="center">
        <div class="resume-item d-flex flex-column flex-md-row mb-5 col-lg-10" style="text-align: left;">
          <div class="resume-content mr-auto">
            <h2 class="mb-1">ABOUT THIS PROJECT</h2>
            <br>
            <p>
                Fragile Families Challenge is a predictive modeling challenge that aims to
                improve lives of disadvantaged children by unconvering factors that affect
                their wellbeing. Princeton University and Columbia University conducted a 15 year
                longitudinal study following a cohort of 5000 children of unmarried couples,
                also known as fragile famlies. The study gathered data on families' living
                condition, financial status, and physical and mental wellbeing from the children's brith year
                through their 15th year. The goal of the challenge is to create a machine learning model
                that predicts 6 outcomes in the final year.

                Among 6 outcome variables, I chose to limit my scope to 3 categorical variables.
                They are...
            </p>
            <ul>
                <li><b>Eviction</b>: whether or not the family was evicted from its home or apartment for
                not paying the rent or mortgage</li>
                <li><b>Layoff</b>: whether or not the primary caregiver of the child has been laid off
                from his or her employer for any time</li>
                <li><b>Job Training</b>: whether or not the primary caregiver of the child has taken
                classes to improve his or her job skills</li>
            </ul>
            <p>
                In this project, I built models that predict the above three variables, and
                interpreted the model output.
            </p>
            <br>
            <h3 class="mb-0">DATA</h3>
            <br>
            <p>
                The dataset provided by Princeton University is a large collection of 4242 observations
                with 13026 variables. These variables are responses from the
                original series of surveys. They can be categorized by respondents: child, mother,
                father, teacher, primary caregiver, in-home survey, in-home observation, etc. Among 4242
                observations of children, FFC provides outcome variables for 2121 of them. The outcome
                variables for the other 2121 observations are used as a holdout dataset to calculate
                validation scores.
            </p>
            <p>
                The non-holdout data is far from perfect- it's very noisy. Because many respondents
                refuse to answer certain questions or miss some surveys, around 55% of data is missing.
                Many of outcome varibles are encoded as missing value as well. The table describes data quality
                of outcome variables in the non-holdout dataset.
            </p>
            <table class="table table-striped">
                <thead class="thead-default">
                    <tr>
                        <th></th>
                        <th>Eviction</th>
                        <th>Layoff</th>
                        <th>Job Training</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Non-missing Count</td>
                        <td>1459</td>
                        <td>1277</td>
                        <td>1461</td>
                    </tr>
                    <tr>
                        <td>Missing Value Count</td>
                        <td>662</td>
                        <td>844</td>
                        <td>660</td>
                    </tr>
                    <tr>
                        <td>True (=1)</td>
                        <td>87</td>
                        <td>267</td>
                        <td>343</td>
                    </tr>
                    <tr>
                        <td>False (=0)</td>
                        <td>710</td>
                        <td>166</td>
                        <td>458</td>
                    </tr>
                    <tr>
                        <td>Proportion of True Labels</td>
                        <td>0.059630</td>
                        <td>0.209084</td>
                        <td>0.234771</td>
                    </tr>
                    <tr>
                        <td>Mode</td>
                        <td>0</td>
                        <td>0</td>
                        <td>0</td>
                    </tr>
                </tbody>
            </table>
            <br>
            <h3 class="mb-0">Methods</h3>
            <br>
            <p>
                Because the original feature space is computationally expensive and may cause overfitting,
                I used the following methods to improve the quality of training data.
            </p>
            <ol>
                <li>
                    <b>Custom Imputation</b>: a lot of variables in the data are duplicates because
                    the survey asks the same set of questions to both parents and records their responses
                    as separate variables. However, since all outcome variables are constructed based on
                    primary caregivers' responses, I combined and cross-referenced both mothers and fathers'
                    responses to generate clean data on primary caregivers. To simplify the process, I assumed that
                    children did not switch their primary caregivers. If the primary caregiver's reponse was missing,
                    I used the other parent's response isntead.
                </li>
                <li>
                    <b>Feature Selection</b>:First, for features that either remain constant over time or change
                    by a fixed amount, I only kept responses from the 9th year (last available year before the 15th).
                    Then, I removed features whose response rates were lower than 80%. Finally, I identified 50
                    most informative features by selecting thoese with high chi-square scores of independence.
                </li>
                <li>
                    <b>Feature Standardization</b>:features were scaled by subtracting the mean and dividing by unit variance.
                </li>
                <li>
                    <b>Principal Component Analysis (PCA)</b>:10 principal components were extracted for final dimensionality reduction.
                </li>
                <li>
                    <b>Oversampling</b>: Because there are a lot more
                    negative outcome variables and positives, I oversampled positive labels from training data.
                    Specifically, I used <b>SMOTE</b>(Synthetic Minority Over- sampling Technique), which creates
                    synthetic data for minority class by randomly choosing neighbors near boundaries
                    generated by KNN algorithm.
                </li>
            </ol>
            <br>
            <h3 class="mb-0">Models</h3>
            <br>
            <p>
                To build a predictive model for each outcome variable, I worked with 3 classifiers.
            </p>
            <ul>
                <li>
                    <b>Logistic Regression</b>: Because the data is very noisy to begin with and has a
                    large feature space, I hypothesized that simple models could perform better than complex ones.
                    Logistic regression is simple and computationally efficient.
                </li>
                <li>
                    <b>Random Forest</b>: RF is effective in preventing overfitting.
                </li>
                <li>
                    <b>Support Vector Machine</b>: SVM can detect potential non-linear relations in data.
                </li>
            </ul>
            <p>
                I tested all three with different levels of preprocessing and performed
                extensive grid search to fine-tune hyper-parameters.
            </p>
            <br>
            <h3 class="mb-0">Evaluation</h3>
            <br>
            <p>
                I used Stratified cross-validation to create training datasets
                representative of the whole dataset. Stratification was an extremely
                important part of the evaluation, because the distribution of binary outcomes
                was very imbalanced (only 6% of eviction, for example), and regular
                cross-validation with random shuffling could result in training datasets
                consisting entirely of the negative class. To compare performances of different models,
                I measured Brier loss, which is the evaluation metric that FFC used to rank submissions.
            </p>
            <center>
                <p>
                    <b>Brier Loss Function</b> = 1/n * &sum;(Y<sub><i>i</i></sub> -  P(Y<sub><i>i</i></sub> = 1) )<sup>2</sup>
                </p>
            </center>
            <br>
            <h3 class="mb-0">Result & Interpretation</h3>
            <br>
            <p> The table below shows validation scores (Brier loss) on the holdout data for
                each outcome variable.
            </p>
            <table class="table table-striped">
                <thead class="thead-default">
                    <tr>
                        <th>Classifier</th>
                        <th>Eviction</th>
                        <th>Layoff</th>
                        <th>Job Training</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Logistic Regression</td>
                        <td>0.05341</td>
                        <td>0.17378</td>
                        <td>0.20197</td>
                    </tr>
                    <tr>
                        <td>Random Forest</td>
                        <td>0.05597</td>
                        <td>0.17478</td>
                        <td>0.21069</td>
                    </tr>
                    <tr>
                        <td>SVM</td>
                        <td>0.05342</td>
                        <td>0.17406</td>
                        <td>0.21069</td>
                    </tr>
                    <tr>
                        <td>SVM w/ SMOTE Oversampling</td>
                        <td>0.05877</td>
                        <td>0.19273</td>
                        <td>0.20461</td>
                    </tr>
                </tbody>
            </table>
            <br>
            <p>
                The table above shows that logistic regression has the minimum loss scores for all
                three outcome variables. To find the most informative features,
                I examined the details of each logistic regression model and found top 3
                principal components with the largest coefficients. Then, for each of those components,
                I extracted 5 variables that had the largest principal axes.
            </p>
            <p>
                In predicting eviction, the best component comprised of 5 variables indicating household income
                and income to poverty ratio. It had a negative coefficient, which intuitively makes sense since
                households with higher income have lower chance of missing rents. Another interesting result was
                the third component, which contained features indicating whether the
                primary caregiver meets deperession criteria or not. It had a positive coefficient-
                caregivers who had mental health issues had a higher chance of getting evicted.
            </p>
            <p>
                The coefficient of the layoff model showed that when caregivers
                were married to their partners, they had a lower chance of being laid off.
                On the other hand, if caregivers lived with grandparents, they were more
                likely to be laid off. Perhaps, caregivers who live with their parents are less motivated
                to stay employed since they can financially rely on others.
            </p>
            <p>
                The best principal component of the model for job training was also made of household income and
                income to poverty ratio, but had a positive coefficient. The positive sign could be explained
                by the fact that those with job training experience were more likely to have higher income.
                Surprisingly, the number of kids and the amount of food expense were negatively correlated
                with job training. The possible explanation that I could think of was that those with multiple kids
                might not have time to participate in such trainings.
            </p>
            <p>
                My takeaway from this project was that simple models often beat complex ones.
                Although I tried multiple iterations of more complex models, simpler models performed better,
                as they were less likely to overfit on the held-out data. Furthermore, for this particular
                project, interpretability was an important criteria, and logistic regression
                produced interpretable results that can offer useful insight to researchers.
            </p>
          </div>
        </div>
      </div>
    </section>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="assets/js/bootstrap.min.js"></script>
  </body>
</html>
